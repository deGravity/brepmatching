{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coincidence_matching import match_parts, get_export_id_types, match_parts_dict\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from automate import Part\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "from brepmatching.data import BRepMatchingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/homes/grail/benjones/.config/onshapecreds.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "str(Path.home().joinpath('.config','onshapecreds.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_f1 = \"https://drive.google.com/file/d/1O7FajIOF5YT_1zW337pm6XE_B2gEhUUc\"\n",
    "gdrive_f2 = \"https://drive.google.com/file/d/1OK4qyuUfSXqA-wd64p_f2pG0rxeeqXGM\"\n",
    "gdrive_f3 = \"https://drive.google.com/file/d/1NnUBCkIUrYBTXvEC8IXGE4V2UNDTG6T6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdrive_wget(FILEID, FILENAME):\n",
    "    return f\"wget --load-cookies /tmp/cookies.txt \\\"https://docs.google.com/uc?export=download&confirm=\" + \\\n",
    "        \"$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate\" + \\\n",
    "        \" 'https://docs.google.com/uc?export=download&id={FILEID}' -O- | sed -rn 's/.*confirm=([0-9A-\" + \\\n",
    "        \"Za-z_]+).*/\\1\\n/p')&id={FILEID}\\\" -O {FILENAME} && rm -rf /tmp/cookies.txt\"\n",
    "def url_to_wget(url, dest):\n",
    "    url = url.split('/d/')[-1]\n",
    "    return gdrive_wget(url, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from apikey.onshape import Onshape\n",
    "from pathlib import Path\n",
    "\n",
    "api = Onshape(stack='https://cad.onshape.com', creds=str(Path.home().joinpath('.config','onshapecreds.json')), logging=False)\n",
    "\n",
    "with ZipFile('../data/TopoandGeoV2FullRunWith100SamplesBaseline.zip','r') as zf:\n",
    "    with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "        variations = pd.read_csv(f)\n",
    "    variations[:100]\n",
    "    variations = variations[variations.fail == 0]\n",
    "\n",
    "    failures = []\n",
    "    written = set()\n",
    "    with ZipFile('../data/redownload.zip','w') as wzf:\n",
    "        for i in tqdm(range(len(variations))):\n",
    "            d = variations.iloc[i]\n",
    "\n",
    "            response_orig = api.request(method='get', path=f'/api/partstudios/d/{d.did}/m/{d.mv_orig}/e/{d.eid}/parasolid', query={'includeExportIds':True})\n",
    "            response_var = api.request(method='get', path=f'/api/partstudios/d/{d.did}/m/{d.mv_var}/e/{d.eid}/parasolid', query={'includeExportIds':True})\n",
    "\n",
    "            if response_orig.status_code != 200 or response_var.status_code != 200:\n",
    "                failures.append(d)\n",
    "                continue\n",
    "            \n",
    "            orig_path = 'data/BrepsWithReference/' + d.ps_orig\n",
    "            var_path = 'data/BrepsWithReference/' + d.ps_var\n",
    "\n",
    "            if orig_path not in written:\n",
    "                with wzf.open(orig_path, 'w') as f:\n",
    "                    f.write(response_orig.text.encode('utf-8'))\n",
    "                written.add(orig_path)\n",
    "            if var_path not in written:\n",
    "                with wzf.open(var_path, 'w') as f:\n",
    "                    f.write(response_var.text.encode('utf-8'))\n",
    "                written.add(var_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topo.zip',\n",
       " 'TopoAndGeoRedownload.zip',\n",
       " 'Geo.zip',\n",
       " 'TopoRedownload.zip',\n",
       " 'TopoAndGeo.zip',\n",
       " 'GeoRedownload.zip',\n",
       " 'GeoRedownload.zip-failures.pickle']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../../brepmatching/Geo.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../../brepmatching/GeoRedownload.zip-failures.pickle','rb') as f:\n",
    "    failures = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cad.onshape.com/documents/13d0516ffa3e9971c6cf76ab/w/f4cf14813a56fd6239cf9094/m/4ca0d1a308e655e35faed811/e/d4d5e78afdef61847ae053bf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures[0].link_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf =  ZipFile('../../../brepmatching/Geo.zip','r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to use ZIP archive that was already closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/baseline/allVariationsWithBaseline.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     variations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(f)\n",
      "File \u001b[0;32m/projects/grail/benjonesnb/code/mambaforge/envs/brepmatching/lib/python3.9/zipfile.py:1498\u001b[0m, in \u001b[0;36mZipFile.open\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpwd is only supported for reading files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp:\n\u001b[0;32m-> 1498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to use ZIP archive that was already closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# Make sure we have an info object\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, ZipInfo):\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;66;03m# 'name' is already an info object\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to use ZIP archive that was already closed"
     ]
    }
   ],
   "source": [
    "with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "    variations = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnames = set(zf.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_exists = []\n",
    "missing = []\n",
    "for p in 'data/baseline/' + variations[variations.fail == 0].baselineNew:\n",
    "    if not p in allnames:\n",
    "        missing.append(p)\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/baseline/ ',\n",
       " 'data/baseline/baselineNew_a11220c88990f369f613901b_4302ae6f3b025356bf93a4ea_ed40c2263fb2591d5c32014f_default_jjieiV3.x_t',\n",
       " 'data/baseline/baselineNew_5820f8088ae1e8103b2bf77b_a30fa1a47133f01cd90768f9_cfbc6fa11e088963e6f40230_default_jjdeqV3.x_t',\n",
       " 'data/baseline/baselineNew_a2eb8f3d2f3907ce4262b2a9_4ff8e3e6143517bbb153119a_f5de5ac33eb7c5717fa5ef6f_default_jjeeiV1.x_t',\n",
       " 'data/baseline/baselineNew_5885d6d43a9ead102a8c5ca9_fd4b966909e746c8e86bbf1d_5d2846a7d52c0da912358721_default_jjeeiV1.x_t',\n",
       " 'data/baseline/baselineNew_28f4025615c0c9f3cd89f3f4_4fae74bb3d3c325aa7ac2925_1ff3a232438852bc665ceb65_default_jjeeiV2.x_t',\n",
       " 'data/baseline/baselineNew_ff226acc53b2673250e3ba83_26b3f6ccc587cdf48c287835_f45e414a58f2a21fec19b385_default_jjdeiV2.x_t',\n",
       " 'data/baseline/baselineNew_504843aac699bc2fccd7d661_8c6f274019fbaccf30a6f171_78b2cdc2a36cbb4eaafb374d_default_jjeeiV1.x_t',\n",
       " 'data/baseline/baselineNew_cc8d096b3a7badb0eab93cd5_4f795c6f7e39bd0dda0bfacf_8aa8948806f4b554a33dcfba_default_jjdeqV1.x_t']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6798"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(variations[variations.fail == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baselineOrig_d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf82d0_5dcc8a4afa039812c247cda8_default_jjeeiV2.x_t'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variations.baselineOrig.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'data/baseline/baselineOrig_d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf82d0_5dcc8a4afa039812c247cda8_default_jjeeiV2.x_t' in allnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_failed = variations[variations.fail == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>did</th>\n",
       "      <th>wid</th>\n",
       "      <th>eid</th>\n",
       "      <th>link_orig</th>\n",
       "      <th>mv_orig</th>\n",
       "      <th>ps_orig</th>\n",
       "      <th>mv_var</th>\n",
       "      <th>ps_var</th>\n",
       "      <th>matchFile</th>\n",
       "      <th>codeVersion</th>\n",
       "      <th>fail</th>\n",
       "      <th>seed</th>\n",
       "      <th>link_var</th>\n",
       "      <th>baselineOrig</th>\n",
       "      <th>baselineNew</th>\n",
       "      <th>baselineMatch</th>\n",
       "      <th>translationFail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...</td>\n",
       "      <td>8f868ab8ff16bbca8df8f914</td>\n",
       "      <td>1be1aa77cc97fec37f31922c</td>\n",
       "      <td>b459723517ecd2dbff382c8e</td>\n",
       "      <td>https://cad.onshape.com/documents/8f868ab8ff16...</td>\n",
       "      <td>2e43732de79d2a8bf3a1bac6</td>\n",
       "      <td>d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...</td>\n",
       "      <td>ce6be646700055037d4f196d</td>\n",
       "      <td>d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...</td>\n",
       "      <td>d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...</td>\n",
       "      <td>MCV2</td>\n",
       "      <td>0</td>\n",
       "      <td>10472</td>\n",
       "      <td>https://cad.onshape.com/documents/8f868ab8ff16...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...   \n",
       "\n",
       "                        did                       wid  \\\n",
       "0  8f868ab8ff16bbca8df8f914  1be1aa77cc97fec37f31922c   \n",
       "\n",
       "                        eid  \\\n",
       "0  b459723517ecd2dbff382c8e   \n",
       "\n",
       "                                           link_orig  \\\n",
       "0  https://cad.onshape.com/documents/8f868ab8ff16...   \n",
       "\n",
       "                    mv_orig  \\\n",
       "0  2e43732de79d2a8bf3a1bac6   \n",
       "\n",
       "                                             ps_orig  \\\n",
       "0  d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...   \n",
       "\n",
       "                     mv_var  \\\n",
       "0  ce6be646700055037d4f196d   \n",
       "\n",
       "                                              ps_var  \\\n",
       "0  d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...   \n",
       "\n",
       "                                           matchFile codeVersion  fail   seed  \\\n",
       "0  d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf8...        MCV2     0  10472   \n",
       "\n",
       "                                            link_var baselineOrig baselineNew  \\\n",
       "0  https://cad.onshape.com/documents/8f868ab8ff16...                            \n",
       "\n",
       "  baselineMatch  translationFail  \n",
       "0                              0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_failed[not_failed.baselineNew == ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████▋                               | 67/100 [01:03<00:31,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "missing_data = []\n",
    "has_missing_export_id = []\n",
    "missing_match_count = []\n",
    "has_missing_matches = []\n",
    "with ZipFile('../../../brepmatching/Geo.zip','r') as zf:\n",
    "    with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "        variations = pd.read_csv(f)\n",
    "    \n",
    "    data = variations\n",
    "    data = data[(data.fail == 0) & (data.baselineNew != ' ') ]\n",
    "    \n",
    "    all_names = set(zf.namelist())\n",
    "    \n",
    "    for i in tqdm(range(100)):#tqdm(range(len(data))):\n",
    "        d = data.iloc[i]\n",
    "        \n",
    "        original_path = 'data/BrepsWithReference/' + d.ps_orig\n",
    "        variation_path = 'data/BrepsWithReference/' + d.ps_var\n",
    "        \n",
    "        baseline_original_path = 'data/baseline/' + d.baselineOrig\n",
    "        baseline_var_path = 'data/baseline/' + d.baselineNew\n",
    "        \n",
    "        path_gt_match = 'data/Matches/' + d.matchFile\n",
    "        path_bl_match = 'data/baseline/' + d.baselineMatch\n",
    "        \n",
    "        data_exists = (original_path in all_names) and \\\n",
    "            (variation_path in all_names) and \\\n",
    "            (baseline_original_path in all_names) and \\\n",
    "            (baseline_var_path in all_names) and \\\n",
    "            (path_gt_match in all_names) and \\\n",
    "            (path_bl_match in all_names)\n",
    "        \n",
    "        # Verify that all data files are in the archive\n",
    "        if not data_exists:\n",
    "            missing_data.append(i)\n",
    "            continue\n",
    "        \n",
    "        # Load parts and match files\n",
    "        with zf.open(original_path,'r') as f:\n",
    "            original_part_data = f.read().decode('utf-8')\n",
    "        with zf.open(variation_path,'r') as f:\n",
    "            variation_part_data = f.read().decode('utf-8')\n",
    "        with zf.open(baseline_original_path, 'r') as f:\n",
    "            baseline_original_data = f.read().decode('utf-8')\n",
    "        with zf.open(baseline_var_path, 'r') as f:\n",
    "            baseline_var_data = f.read().decode('utf-8')\n",
    "        with zf.open(path_gt_match, 'r') as f:\n",
    "            gt_match = json.load(f)\n",
    "        with zf.open(path_bl_match, 'r') as f:\n",
    "            baseline_match = json.load(f)\n",
    "            \n",
    "        \n",
    "        original_ids = get_export_id_types(original_part_data)\n",
    "        variation_ids = get_export_id_types(variation_part_data)\n",
    "        \n",
    "        bl_original_ids = get_export_id_types(baseline_original_data)\n",
    "        bl_variation_ids = get_export_id_types(baseline_var_data)\n",
    "        \n",
    "        # Verify that all referenced export_ids exist in the corresponding files\n",
    "        export_id_missing = False\n",
    "        for v in gt_match.values():\n",
    "            orig_export_id = v['val1']\n",
    "            var_export_id = v['val2']\n",
    "            \n",
    "            if orig_export_id not in original_ids or var_export_id not in variation_ids:\n",
    "                export_id_missing = True\n",
    "        \n",
    "        for v in baseline_match.values():\n",
    "            orig_export_id = v['val1']\n",
    "            var_export_id = v['val2']\n",
    "            \n",
    "            if orig_export_id not in bl_original_ids or var_export_id not in bl_variation_ids:\n",
    "                export_id_missing = True\n",
    "        \n",
    "        if export_id_missing:\n",
    "            has_missing_export_id.append(i)\n",
    "            continue\n",
    "        \n",
    "        # Check that exact matching the baseline variation to the primary variation always works\n",
    "        \n",
    "        var_matching = match_parts(baseline_var_data, variation_part_data, True)\n",
    "        var_matching_dict = match_parts_dict(baseline_var_data, variation_part_data, True)\n",
    "        \n",
    "        n_faces_variation = len([k for k,v in variation_ids.items() if v == 'PK_CLASS_face'])\n",
    "        n_faces_baseline_variation = len([k for k,v in bl_variation_ids.items() if v == 'PK_CLASS_face'])\n",
    "        \n",
    "        n_edges_variation = len([k for k,v in variation_ids.items() if v == 'PK_CLASS_edge'])\n",
    "        n_edges_baseline_variation = len([k for k,v in bl_variation_ids.items() if v == 'PK_CLASS_edge'])\n",
    "        \n",
    "        n_vertices_variation = len([k for k,v in variation_ids.items() if v == 'PK_CLASS_vertex'])\n",
    "        n_vertices_baseline_variation = len([k for k,v in bl_variation_ids.items() if v == 'PK_CLASS_vertex'])\n",
    "        \n",
    "        #assert(len(var_matching.face_matches) == n_faces_variation)\n",
    "        #assert(len(var_matching.face_matches) == n_faces_baseline_variation)\n",
    "        #assert(len(var_matching.edge_matches) == n_edges_variation)\n",
    "        #assert(len(var_matching.edge_matches) == n_edges_baseline_variation)\n",
    "        #assert(len(var_matching.vertex_matches) == n_vertices_variation)\n",
    "        #assert(len(var_matching.vertex_matches) == n_vertices_baseline_variation)\n",
    "        \n",
    "        good_topos = ['PK_CLASS_face', 'PK_CLASS_edge', 'PK_CLASS_vertex']\n",
    "        \n",
    "        n_matches_missing = 0\n",
    "        for v in baseline_match.values():\n",
    "            var_export_id = v['val2']\n",
    "            if bl_variation_ids[var_export_id] in good_topos:\n",
    "                if var_export_id not in var_matching_dict:\n",
    "                    n_matches_missing += 1\n",
    "        \n",
    "        missing_match_count.append(n_matches_missing)\n",
    "        \n",
    "        if n_matches_missing > 0:\n",
    "            has_missing_matches.append(i)\n",
    "            \n",
    "        if n_matches_missing > 2:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_part = Part(variation_part_data)\n",
    "baseline_variation_part = Part(baseline_var_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f5664f8ed643d39f9f1b528c9a03a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0, 0.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7f66e0e8dc10>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.plot(variation_part.mesh.V, variation_part.mesh.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_edges_baseline_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(var_matching.edge_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_export_types = get_export_id_types(original_part_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_ids_original = [v['val1'] for v in gt_match.values()]\n",
    "match_ids_variation = [v['val2'] for v in gt_match.values()]\n",
    "bl_match_ids_original = [v['val1'] for v in baseline_match.values()]\n",
    "bl_match_ids_variation = [v['val2'] for v in baseline_match.values()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([x in original_export_types for x in match_ids_original])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing var: KF4C\n",
      "missing orig: KFcD\n",
      "missing var: KFsE\n",
      "missing orig: KF8E\n",
      "missing var: KJBD\n",
      "missing orig: JF9\n",
      "missing orig: KFdD\n",
      "missing var: KFJG\n",
      "missing orig: KFhG\n",
      "missing var: KF9G\n",
      "missing var: KFZH\n",
      "missing var: KFJK\n",
      "missing orig: KFlL\n",
      "missing var: KFlL\n",
      "missing var: KJBB\n",
      "missing var: KI2C\n",
      "missing orig: JF2\n",
      "missing var: KIaD\n",
      "missing var: KIKB\n",
      "missing var: KIaB\n",
      "missing var: KIqB\n",
      "missing var: KI6B\n",
      "missing var: KIKC\n",
      "missing var: KFGE\n",
      "missing var: KFaE\n",
      "missing var: KI+D\n",
      "missing baseline var KF8B\n",
      "missing baseline orig: KFkD\n",
      "missing baseline orig: KFME\n",
      "missing baseline var KFRD\n",
      "missing baseline orig: KFVD\n",
      "missing baseline var KFRE\n",
      "missing baseline var KFFF\n",
      "missing baseline var KFtF\n",
      "missing baseline orig: KF9F\n",
      "missing baseline var KFJG\n",
      "missing baseline var KFdG\n",
      "missing baseline orig: KF9G\n",
      "missing baseline orig: KFNH\n",
      "missing baseline orig: KFBI\n",
      "missing baseline orig: KFVI\n",
      "missing baseline orig: KFJJ\n",
      "missing baseline var KFRJ\n",
      "missing baseline var KFxK\n",
      "missing baseline orig: KFNM\n",
      "missing baseline orig: KFxM\n",
      "missing baseline orig: KFZN\n",
      "missing baseline var KFVO\n",
      "missing baseline orig: KFlP\n",
      "missing baseline orig: KF9P\n",
      "missing baseline var JFu\n",
      "missing baseline orig: JFy\n",
      "missing baseline orig: KFmC\n"
     ]
    }
   ],
   "source": [
    "for v in gt_match.values():\n",
    "    orig_export_id = v['val1']\n",
    "    var_export_id = v['val2']\n",
    "\n",
    "    if orig_export_id not in original_part_data:\n",
    "        print(f'missing orig: {orig_export_id}')\n",
    "    \n",
    "    if var_export_id not in variation_part_data:\n",
    "        print(f'missing var: {var_export_id}')\n",
    "\n",
    "for v in baseline_match.values():\n",
    "    bl_orig_export_id = v['val1']\n",
    "    bl_var_export_id = v['val2']\n",
    "\n",
    "    if bl_orig_export_id not in baseline_original_data:\n",
    "        print(f'missing baseline orig: {bl_orig_export_id}')\n",
    "    \n",
    "    if bl_var_export_id not in baseline_var_data:\n",
    "        print(f'missing baseline var {bl_var_export_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████▏                                    | 61/99 [00:21<00:13,  2.78it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'data/baseline/baselineOrig_a11220c88990f369f613901b_4302ae6f3b025356bf93a4ea_ed40c2263fb2591d5c32014f_default_jjieiV3.x_t' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(path_a,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     41\u001b[0m     part_data_a \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_b\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     43\u001b[0m     part_data_b \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(path_c,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/projects/grail/benjonesnb/code/mambaforge/envs/brepmatching/lib/python3.9/zipfile.py:1511\u001b[0m, in \u001b[0;36mZipFile.open\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     zinfo\u001b[38;5;241m.\u001b[39m_compresslevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompresslevel\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;66;03m# Get info object for name\u001b[39;00m\n\u001b[0;32m-> 1511\u001b[0m     zinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_to_write(zinfo, force_zip64\u001b[38;5;241m=\u001b[39mforce_zip64)\n",
      "File \u001b[0;32m/projects/grail/benjonesnb/code/mambaforge/envs/brepmatching/lib/python3.9/zipfile.py:1438\u001b[0m, in \u001b[0;36mZipFile.getinfo\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1436\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNameToInfo\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1439\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere is no item named \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m in the archive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[0;31mKeyError\u001b[0m: \"There is no item named 'data/baseline/baselineOrig_a11220c88990f369f613901b_4302ae6f3b025356bf93a4ea_ed40c2263fb2591d5c32014f_default_jjieiV3.x_t' in the archive\""
     ]
    }
   ],
   "source": [
    "#with ZipFile('../data/TopoandGeoV2FullRunWith100SamplesBaseline.zip','r') as zf:\n",
    "\n",
    "#with ZipFile('../data/GeoV2FullRunBaseline.zip','r') as zf:\n",
    "\n",
    "with ZipFile('../../../brepmatching/Geo.zip','r') as zf:\n",
    "\n",
    "\n",
    "    with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "        variations = pd.read_csv(f)\n",
    "\n",
    "    data = variations[:100]\n",
    "    data = data[(data.fail == 0) & (data.baselineNew != ' ') ]\n",
    "\n",
    "\n",
    "#with ZipFile('../../../brepmatching/GeoRedownload.zip','r') as zf:\n",
    "    \n",
    "    incomplete = []\n",
    "    incompatible = []\n",
    "    diff_geo = []\n",
    "\n",
    "    m_f = []\n",
    "    m_e = []\n",
    "    m_v = []\n",
    "\n",
    "    A = []\n",
    "    B = []\n",
    "    M = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        d = data.iloc[i]\n",
    "        path_a = 'data/BrepsWithReference/' + d.ps_orig\n",
    "        path_b = 'data/baseline/' + d.baselineOrig\n",
    "\n",
    "        path_c = 'data/BrepsWithReference/' + d.ps_var\n",
    "        path_d = 'data/baseline/' + d.baselineNew\n",
    "        \n",
    "        path_gt_match = 'data/Matches/' + d.matchFile\n",
    "        path_bl_match = 'data/baseline/' + d.baselineMatch\n",
    "\n",
    "        with zf.open(path_a,'r') as f:\n",
    "            part_data_a = f.read().decode('utf-8')\n",
    "        with zf.open(path_b,'r') as f:\n",
    "            part_data_b = f.read().decode('utf-8')\n",
    "        with zf.open(path_c,'r') as f:\n",
    "            part_data_c = f.read().decode('utf-8')\n",
    "        with zf.open(path_d,'r') as f:\n",
    "            part_data_d = f.read().decode('utf-8')\n",
    "        with zf.open(path_gt_match,'r') as f:\n",
    "            gt_match = json.load(f)\n",
    "        with zf.open(path_bl_match, 'r') as f:\n",
    "            bl_match = json.load(f)\n",
    "\n",
    "        p_a = Part(part_data_a)\n",
    "        p_b = Part(part_data_b)\n",
    "        p_c = Part(part_data_c)\n",
    "        p_d = Part(part_data_d)\n",
    "\n",
    "        parts_have_same_num = len(p_a.brep.nodes.faces) == len(p_b.brep.nodes.faces) and len(p_a.brep.nodes.edges) == len(p_b.brep.nodes.edges) and len(p_a.brep.nodes.vertices) == len(p_b.brep.nodes.vertices)\n",
    "\n",
    "        matching = match_parts(part_data_a, part_data_b, True)\n",
    "        complete_matching = len(matching.face_matches) == len(p_a.brep.nodes.faces) and len(matching.edge_matches) == len(p_a.brep.nodes.edges) and len(matching.vertex_matches) == len(p_a.brep.nodes.vertices)\n",
    "\n",
    "        n_face_matches_missing = -len(matching.face_matches) + len(p_a.brep.nodes.faces)\n",
    "        n_edge_matches_missing = -len(matching.edge_matches) + len(p_a.brep.nodes.edges)\n",
    "        n_vertex_matches_missing =-len(matching.vertex_matches) + len(p_a.brep.nodes.vertices)\n",
    "\n",
    "        m_f.append(n_face_matches_missing)\n",
    "        m_e.append(n_edge_matches_missing)\n",
    "        m_v.append(n_vertex_matches_missing)\n",
    "\n",
    "        exact_geo = (p_a.mesh.V == p_b.mesh.V).all()\n",
    "\n",
    "        if not parts_have_same_num:\n",
    "            incompatible.append(i)\n",
    "        if not complete_matching:\n",
    "            incomplete.append(i)\n",
    "\n",
    "        if not exact_geo:\n",
    "            diff_geo.append(i)\n",
    "\n",
    "        A.append(p_a)\n",
    "        B.append(p_b)\n",
    "        M.append(matching)\n",
    "\n",
    "\n",
    "        # Exact matching of originals seems to work (and the tesselations are mostly identical!)\n",
    "        # Let's see how well we match the versions with variations:\n",
    "\n",
    "        var_matching = match_parts(part_data_c, part_data_d, True)\n",
    "\n",
    "        id_classes_a = get_export_id_types(part_data_a)\n",
    "        id_classes_b = get_export_id_types(part_data_b)\n",
    "        id_classes_c = get_export_id_types(part_data_c)\n",
    "        id_classes_d = get_export_id_types(part_data_d)\n",
    "\n",
    "        good_classes = ['PK_CLASS_face', 'PK_CLASS_edge', 'PK_CLASS_vertex']\n",
    "\n",
    "        #gt_topo_matches = [(v['val1'],v['val1']) for k,v in gt_match.items() if id_classes_a[v['val1']] in good_classes and id_classes_c[v['val2']] in good_classes]\n",
    "        bl_topo_matches = [(v['val1'],v['val1']) for k,v in bl_match.items() if id_classes_b[v['val1']] in good_classes and id_classes_d[v['val2']] in good_classes]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'JF4' in part_data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.link_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_b = Part(part_data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([e.export_id for e in part_b.brep.nodes.edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([k for k,v in id_classes_b.items() if v == 'PK_CLASS_edge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_classes_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.link_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.baselineMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apikey.onshape import Onshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api.request(method='get', path=f'/api/partstudios/d/{d.did}/m/{d.mv_var}/e/{d.eid}/parasolid', query={'includeExportIds':True})#, query={'partIds':['JFD'],'includeExportIds':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text == part_data_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.mv_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.eid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_classes_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brepmatching",
   "language": "python",
   "name": "brepmatching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "87142ede9042d04f934af2ae171157c789b6aae7f3fe10d44294683606c509dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
