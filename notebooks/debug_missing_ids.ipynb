{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coincidence_matching import match_parts, get_export_id_types, match_parts_dict\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from automate import Part\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "from brepmatching.data import BRepMatchingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "str(Path.home().joinpath('.config','onshapecreds.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_f1 = \"https://drive.google.com/file/d/1O7FajIOF5YT_1zW337pm6XE_B2gEhUUc\"\n",
    "gdrive_f2 = \"https://drive.google.com/file/d/1OK4qyuUfSXqA-wd64p_f2pG0rxeeqXGM\"\n",
    "gdrive_f3 = \"https://drive.google.com/file/d/1NnUBCkIUrYBTXvEC8IXGE4V2UNDTG6T6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdrive_wget(FILEID, FILENAME):\n",
    "    return f\"wget --load-cookies /tmp/cookies.txt \\\"https://docs.google.com/uc?export=download&confirm=\" + \\\n",
    "        \"$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate\" + \\\n",
    "        \" 'https://docs.google.com/uc?export=download&id={FILEID}' -O- | sed -rn 's/.*confirm=([0-9A-\" + \\\n",
    "        \"Za-z_]+).*/\\1\\n/p')&id={FILEID}\\\" -O {FILENAME} && rm -rf /tmp/cookies.txt\"\n",
    "def url_to_wget(url, dest):\n",
    "    url = url.split('/d/')[-1]\n",
    "    return gdrive_wget(url, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from apikey.onshape import Onshape\n",
    "from pathlib import Path\n",
    "\n",
    "api = Onshape(stack='https://cad.onshape.com', creds=str(Path.home().joinpath('.config','onshapecreds.json')), logging=False)\n",
    "\n",
    "with ZipFile('../data/TopoandGeoV2FullRunWith100SamplesBaseline.zip','r') as zf:\n",
    "    with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "        variations = pd.read_csv(f)\n",
    "    variations[:100]\n",
    "    variations = variations[variations.fail == 0]\n",
    "\n",
    "    failures = []\n",
    "    written = set()\n",
    "    with ZipFile('../data/redownload.zip','w') as wzf:\n",
    "        for i in tqdm(range(len(variations))):\n",
    "            d = variations.iloc[i]\n",
    "\n",
    "            response_orig = api.request(method='get', path=f'/api/partstudios/d/{d.did}/m/{d.mv_orig}/e/{d.eid}/parasolid', query={'includeExportIds':True})\n",
    "            response_var = api.request(method='get', path=f'/api/partstudios/d/{d.did}/m/{d.mv_var}/e/{d.eid}/parasolid', query={'includeExportIds':True})\n",
    "\n",
    "            if response_orig.status_code != 200 or response_var.status_code != 200:\n",
    "                failures.append(d)\n",
    "                continue\n",
    "            \n",
    "            orig_path = 'data/BrepsWithReference/' + d.ps_orig\n",
    "            var_path = 'data/BrepsWithReference/' + d.ps_var\n",
    "\n",
    "            if orig_path not in written:\n",
    "                with wzf.open(orig_path, 'w') as f:\n",
    "                    f.write(response_orig.text.encode('utf-8'))\n",
    "                written.add(orig_path)\n",
    "            if var_path not in written:\n",
    "                with wzf.open(var_path, 'w') as f:\n",
    "                    f.write(response_var.text.encode('utf-8'))\n",
    "                written.add(var_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../../../brepmatching/Geo.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../../brepmatching/GeoRedownload.zip-failures.pickle','rb') as f:\n",
    "    failures = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures[0].link_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf =  ZipFile('../../../brepmatching/Geo.zip','r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "    variations = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnames = set(zf.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exists = []\n",
    "missing = []\n",
    "for p in 'data/baseline/' + variations[variations.fail == 0].baselineNew:\n",
    "    if not p in allnames:\n",
    "        missing.append(p)\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variations[variations.fail == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations.baselineOrig.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'data/baseline/baselineOrig_d65b13c6430b30819cb6b169_ab94f8bbfa837d13accf82d0_5dcc8a4afa039812c247cda8_default_jjeeiV2.x_t' in allnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_failed = variations[variations.fail == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_failed[not_failed.baselineNew == ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = []\n",
    "has_missing_export_id = []\n",
    "missing_match_count = []\n",
    "has_missing_matches = []\n",
    "\n",
    "#data_path = '../../../brepmatching/Geo.zip'\n",
    "data_path = '../data/GeoV2FullRunBaseline.zip'\n",
    "\n",
    "with ZipFile(data_path,'r') as zf:\n",
    "    with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "        variations = pd.read_csv(f)\n",
    "    \n",
    "    data = variations\n",
    "    data = data[(data.fail == 0) & (data.baselineNew != ' ') ]\n",
    "    \n",
    "    all_names = set(zf.namelist())\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        d = data.iloc[i]\n",
    "        \n",
    "        original_path = 'data/BrepsWithReference/' + d.ps_orig\n",
    "        variation_path = 'data/BrepsWithReference/' + d.ps_var\n",
    "        \n",
    "        baseline_original_path = 'data/baseline/' + d.baselineOrig\n",
    "        baseline_var_path = 'data/baseline/' + d.baselineNew\n",
    "        \n",
    "        path_gt_match = 'data/Matches/' + d.matchFile\n",
    "        path_bl_match = 'data/baseline/' + d.baselineMatch\n",
    "        \n",
    "        data_exists = (original_path in all_names) and \\\n",
    "            (variation_path in all_names) and \\\n",
    "            (baseline_original_path in all_names) and \\\n",
    "            (baseline_var_path in all_names) and \\\n",
    "            (path_gt_match in all_names) and \\\n",
    "            (path_bl_match in all_names)\n",
    "        \n",
    "        # Verify that all data files are in the archive\n",
    "        if not data_exists:\n",
    "            missing_data.append(i)\n",
    "            continue\n",
    "        \n",
    "        # Load parts and match files\n",
    "        with zf.open(original_path,'r') as f:\n",
    "            original_part_data = f.read().decode('utf-8')\n",
    "        with zf.open(variation_path,'r') as f:\n",
    "            variation_part_data = f.read().decode('utf-8')\n",
    "        with zf.open(baseline_original_path, 'r') as f:\n",
    "            baseline_original_data = f.read().decode('utf-8')\n",
    "        with zf.open(baseline_var_path, 'r') as f:\n",
    "            baseline_var_data = f.read().decode('utf-8')\n",
    "        with zf.open(path_gt_match, 'r') as f:\n",
    "            gt_match = json.load(f)\n",
    "        with zf.open(path_bl_match, 'r') as f:\n",
    "            baseline_match = json.load(f)\n",
    "            \n",
    "        \n",
    "        original_ids = get_export_id_types(original_part_data)\n",
    "        variation_ids = get_export_id_types(variation_part_data)\n",
    "        \n",
    "        bl_original_ids = get_export_id_types(baseline_original_data)\n",
    "        bl_variation_ids = get_export_id_types(baseline_var_data)\n",
    "        \n",
    "        # Verify that all referenced export_ids exist in the corresponding files\n",
    "        export_id_missing = False\n",
    "        for v in gt_match.values():\n",
    "            orig_export_id = v['val1']\n",
    "            var_export_id = v['val2']\n",
    "            \n",
    "            if orig_export_id not in original_ids or var_export_id not in variation_ids:\n",
    "                export_id_missing = True\n",
    "        \n",
    "        for v in baseline_match.values():\n",
    "            orig_export_id = v['val1']\n",
    "            var_export_id = v['val2']\n",
    "            \n",
    "            if orig_export_id not in bl_original_ids or var_export_id not in bl_variation_ids:\n",
    "                export_id_missing = True\n",
    "        \n",
    "        if export_id_missing:\n",
    "            has_missing_export_id.append(i)\n",
    "            continue\n",
    "        \n",
    "        # Check that exact matching the baseline variation to the primary variation always works\n",
    "        \n",
    "        var_matching = match_parts(baseline_var_data, variation_part_data, True)\n",
    "        var_matching_dict = match_parts_dict(baseline_var_data, variation_part_data, True)\n",
    "        \n",
    "        n_faces_variation = len([k for k,v in variation_ids.items() if v == 'PK_CLASS_face'])\n",
    "        n_faces_baseline_variation = len([k for k,v in bl_variation_ids.items() if v == 'PK_CLASS_face'])\n",
    "        \n",
    "        n_edges_variation = len([k for k,v in variation_ids.items() if v == 'PK_CLASS_edge'])\n",
    "        n_edges_baseline_variation = len([k for k,v in bl_variation_ids.items() if v == 'PK_CLASS_edge'])\n",
    "        \n",
    "        n_vertices_variation = len([k for k,v in variation_ids.items() if v == 'PK_CLASS_vertex'])\n",
    "        n_vertices_baseline_variation = len([k for k,v in bl_variation_ids.items() if v == 'PK_CLASS_vertex'])\n",
    "        \n",
    "        #assert(len(var_matching.face_matches) == n_faces_variation)\n",
    "        #assert(len(var_matching.face_matches) == n_faces_baseline_variation)\n",
    "        #assert(len(var_matching.edge_matches) == n_edges_variation)\n",
    "        #assert(len(var_matching.edge_matches) == n_edges_baseline_variation)\n",
    "        #assert(len(var_matching.vertex_matches) == n_vertices_variation)\n",
    "        #assert(len(var_matching.vertex_matches) == n_vertices_baseline_variation)\n",
    "        \n",
    "        good_topos = ['PK_CLASS_face', 'PK_CLASS_edge', 'PK_CLASS_vertex']\n",
    "        \n",
    "        n_matches_missing = 0\n",
    "        for v in baseline_match.values():\n",
    "            var_export_id = v['val2']\n",
    "            if bl_variation_ids[var_export_id] in good_topos:\n",
    "                if var_export_id not in var_matching_dict:\n",
    "                    n_matches_missing += 1\n",
    "        \n",
    "        missing_match_count.append(n_matches_missing)\n",
    "        \n",
    "        if n_matches_missing > 0:\n",
    "            has_missing_matches.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmc = np.array(missing_match_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mmc > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(has_missing_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(var_matching.edge_matches) == n_edges_variation)\n",
    "print(len(var_matching.edge_matches) == n_edges_baseline_variation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_part = Part(variation_part_data)\n",
    "baseline_variation_part = Part(baseline_var_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_missing_edge_indices = [i for i,e in enumerate(baseline_variation_part.brep.nodes.edges) if e.export_id not in var_matching_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_edge_indices = [i for i,e in enumerate(variation_part.brep.nodes.edges) if e.export_id not in var_matching_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_edge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_variation_part.brep.nodes.edges[8].function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_missing_edge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2t = variation_part.mesh_topology.edge_to_topology\n",
    "V = variation_part.mesh.V\n",
    "F = variation_part.mesh.F\n",
    "E = []\n",
    "for i in range(e2t.shape[0]):\n",
    "    for j in range(e2t.shape[1]):\n",
    "        if e2t[i,j] in missing_edge_indices:\n",
    "            E.append((F[i,(j-1)%3], F[i,j]))\n",
    "E = np.array(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2t = baseline_variation_part.mesh_topology.edge_to_topology\n",
    "V_b = baseline_variation_part.mesh.V\n",
    "F_b = baseline_variation_part.mesh.F\n",
    "E_b = []\n",
    "for i in range(e2t.shape[0]):\n",
    "    for j in range(e2t.shape[1]):\n",
    "        if e2t[i,j] in bl_missing_edge_indices:\n",
    "            E_b.append((F[i,(j-1)%3], F[i,j]))\n",
    "E_b = np.array(E_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('meshes.nzp',V=V,F=F,E=E,V_b=V_b,F_b=F_b,E_b=E_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('E.npy',E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(variation_part.mesh.V, variation_part.mesh.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(baseline_variation_part.mesh.V, baseline_variation_part.mesh.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_part.mesh.V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igl.write_obj('baseline_var.obj', baseline_variation_part.mesh.V, baseline_variation_part.mesh.F)\n",
    "igl.write_obj('var.obj', variation_part.mesh.V, variation_part.mesh.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges_baseline_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(var_matching.edge_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_export_types = get_export_id_types(original_part_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_ids_original = [v['val1'] for v in gt_match.values()]\n",
    "match_ids_variation = [v['val2'] for v in gt_match.values()]\n",
    "bl_match_ids_original = [v['val1'] for v in baseline_match.values()]\n",
    "bl_match_ids_variation = [v['val2'] for v in baseline_match.values()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all([x in original_export_types for x in match_ids_original])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in gt_match.values():\n",
    "    orig_export_id = v['val1']\n",
    "    var_export_id = v['val2']\n",
    "\n",
    "    if orig_export_id not in original_part_data:\n",
    "        print(f'missing orig: {orig_export_id}')\n",
    "    \n",
    "    if var_export_id not in variation_part_data:\n",
    "        print(f'missing var: {var_export_id}')\n",
    "\n",
    "for v in baseline_match.values():\n",
    "    bl_orig_export_id = v['val1']\n",
    "    bl_var_export_id = v['val2']\n",
    "\n",
    "    if bl_orig_export_id not in baseline_original_data:\n",
    "        print(f'missing baseline orig: {bl_orig_export_id}')\n",
    "    \n",
    "    if bl_var_export_id not in baseline_var_data:\n",
    "        print(f'missing baseline var {bl_var_export_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with ZipFile('../data/TopoandGeoV2FullRunWith100SamplesBaseline.zip','r') as zf:\n",
    "\n",
    "#with ZipFile('../data/GeoV2FullRunBaseline.zip','r') as zf:\n",
    "\n",
    "with ZipFile('../../../brepmatching/Geo.zip','r') as zf:\n",
    "\n",
    "\n",
    "    with zf.open('data/baseline/allVariationsWithBaseline.csv','r') as f:\n",
    "        variations = pd.read_csv(f)\n",
    "\n",
    "    data = variations[:100]\n",
    "    data = data[(data.fail == 0) & (data.baselineNew != ' ') ]\n",
    "\n",
    "\n",
    "#with ZipFile('../../../brepmatching/GeoRedownload.zip','r') as zf:\n",
    "    \n",
    "    incomplete = []\n",
    "    incompatible = []\n",
    "    diff_geo = []\n",
    "\n",
    "    m_f = []\n",
    "    m_e = []\n",
    "    m_v = []\n",
    "\n",
    "    A = []\n",
    "    B = []\n",
    "    M = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        d = data.iloc[i]\n",
    "        path_a = 'data/BrepsWithReference/' + d.ps_orig\n",
    "        path_b = 'data/baseline/' + d.baselineOrig\n",
    "\n",
    "        path_c = 'data/BrepsWithReference/' + d.ps_var\n",
    "        path_d = 'data/baseline/' + d.baselineNew\n",
    "        \n",
    "        path_gt_match = 'data/Matches/' + d.matchFile\n",
    "        path_bl_match = 'data/baseline/' + d.baselineMatch\n",
    "\n",
    "        with zf.open(path_a,'r') as f:\n",
    "            part_data_a = f.read().decode('utf-8')\n",
    "        with zf.open(path_b,'r') as f:\n",
    "            part_data_b = f.read().decode('utf-8')\n",
    "        with zf.open(path_c,'r') as f:\n",
    "            part_data_c = f.read().decode('utf-8')\n",
    "        with zf.open(path_d,'r') as f:\n",
    "            part_data_d = f.read().decode('utf-8')\n",
    "        with zf.open(path_gt_match,'r') as f:\n",
    "            gt_match = json.load(f)\n",
    "        with zf.open(path_bl_match, 'r') as f:\n",
    "            bl_match = json.load(f)\n",
    "\n",
    "        p_a = Part(part_data_a)\n",
    "        p_b = Part(part_data_b)\n",
    "        p_c = Part(part_data_c)\n",
    "        p_d = Part(part_data_d)\n",
    "\n",
    "        parts_have_same_num = len(p_a.brep.nodes.faces) == len(p_b.brep.nodes.faces) and len(p_a.brep.nodes.edges) == len(p_b.brep.nodes.edges) and len(p_a.brep.nodes.vertices) == len(p_b.brep.nodes.vertices)\n",
    "\n",
    "        matching = match_parts(part_data_a, part_data_b, True)\n",
    "        complete_matching = len(matching.face_matches) == len(p_a.brep.nodes.faces) and len(matching.edge_matches) == len(p_a.brep.nodes.edges) and len(matching.vertex_matches) == len(p_a.brep.nodes.vertices)\n",
    "\n",
    "        n_face_matches_missing = -len(matching.face_matches) + len(p_a.brep.nodes.faces)\n",
    "        n_edge_matches_missing = -len(matching.edge_matches) + len(p_a.brep.nodes.edges)\n",
    "        n_vertex_matches_missing =-len(matching.vertex_matches) + len(p_a.brep.nodes.vertices)\n",
    "\n",
    "        m_f.append(n_face_matches_missing)\n",
    "        m_e.append(n_edge_matches_missing)\n",
    "        m_v.append(n_vertex_matches_missing)\n",
    "\n",
    "        exact_geo = (p_a.mesh.V == p_b.mesh.V).all()\n",
    "\n",
    "        if not parts_have_same_num:\n",
    "            incompatible.append(i)\n",
    "        if not complete_matching:\n",
    "            incomplete.append(i)\n",
    "\n",
    "        if not exact_geo:\n",
    "            diff_geo.append(i)\n",
    "\n",
    "        A.append(p_a)\n",
    "        B.append(p_b)\n",
    "        M.append(matching)\n",
    "\n",
    "\n",
    "        # Exact matching of originals seems to work (and the tesselations are mostly identical!)\n",
    "        # Let's see how well we match the versions with variations:\n",
    "\n",
    "        var_matching = match_parts(part_data_c, part_data_d, True)\n",
    "\n",
    "        id_classes_a = get_export_id_types(part_data_a)\n",
    "        id_classes_b = get_export_id_types(part_data_b)\n",
    "        id_classes_c = get_export_id_types(part_data_c)\n",
    "        id_classes_d = get_export_id_types(part_data_d)\n",
    "\n",
    "        good_classes = ['PK_CLASS_face', 'PK_CLASS_edge', 'PK_CLASS_vertex']\n",
    "\n",
    "        #gt_topo_matches = [(v['val1'],v['val1']) for k,v in gt_match.items() if id_classes_a[v['val1']] in good_classes and id_classes_c[v['val2']] in good_classes]\n",
    "        bl_topo_matches = [(v['val1'],v['val1']) for k,v in bl_match.items() if id_classes_b[v['val1']] in good_classes and id_classes_d[v['val2']] in good_classes]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'JF4' in part_data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.link_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_b = Part(part_data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([e.export_id for e in part_b.brep.nodes.edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([k for k,v in id_classes_b.items() if v == 'PK_CLASS_edge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_classes_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.link_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.baselineMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apikey.onshape import Onshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api.request(method='get', path=f'/api/partstudios/d/{d.did}/m/{d.mv_var}/e/{d.eid}/parasolid', query={'includeExportIds':True})#, query={'partIds':['JFD'],'includeExportIds':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text == part_data_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.mv_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.eid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_classes_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "87142ede9042d04f934af2ae171157c789b6aae7f3fe10d44294683606c509dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
